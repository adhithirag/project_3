{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ae0035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'lat': {'dimensions': ('lat',), 'shape': (90,)},\n",
       "  'lon': {'dimensions': ('lon',), 'shape': (180,)},\n",
       "  'time': {'dimensions': ('time',), 'shape': (1728,)},\n",
       "  'time_bnds': {'dimensions': ('time', 'nv'), 'shape': (1728, 2)},\n",
       "  'tempanomaly': {'dimensions': ('time', 'lat', 'lon'),\n",
       "   'shape': (1728, 90, 180)}},\n",
       " {'title': 'GISTEMP Surface Temperature Analysis',\n",
       "  'institution': 'NASA Goddard Institute for Space Studies',\n",
       "  'source': 'http://data.giss.nasa.gov/gistemp/',\n",
       "  'Conventions': 'CF-1.6',\n",
       "  'history': 'Created 2024-01-10 11:48:34 by SBBX_to_nc 2.0 - ILAND=250,  IOCEAN=none,     Base: 1951-1980'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code was generated by Chat GPT. Asking it to convert a .nc file into .json. \n",
    "#Chat GPT first needed to check the structure\n",
    "\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Load the NetCDF file to check its structure\n",
    "nc_file_path = 'gistemp250_GHCNv4.nc'\n",
    "dataset = nc.Dataset(nc_file_path)\n",
    "\n",
    "# Gather basic information about the file to decide on the conversion strategy\n",
    "variables_info = {var: {'dimensions': dataset.variables[var].dimensions, 'shape': dataset.variables[var].shape} for var in dataset.variables.keys()}\n",
    "global_attributes = {attr: dataset.getncattr(attr) for attr in dataset.ncattrs()}\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "variables_info, global_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c8d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT Generated code to convert .nc into .json. \n",
    "# Chat GPT creates a new file \n",
    "import netCDF4 as nc\n",
    "import json\n",
    "\n",
    "def convert_nc_to_json(nc_file_path, json_file_path):\n",
    "    dataset = nc.Dataset(nc_file_path)\n",
    "    \n",
    "    # Prepare a dictionary to hold the extracted data\n",
    "    data_dict = {\n",
    "        \"title\": dataset.title,\n",
    "        \"institution\": dataset.institution,\n",
    "        \"source\": dataset.source,\n",
    "        \"Conventions\": dataset.Conventions,\n",
    "        \"history\": dataset.history,\n",
    "        \"data\": {\n",
    "            \"tempanomaly\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract time values (assuming they are in a convertible format)\n",
    "    time = dataset.variables['time'][:]\n",
    "    latitudes = dataset.variables['lat'][:]\n",
    "    longitudes = dataset.variables['lon'][:]\n",
    "    tempanomalies = dataset.variables['tempanomaly'][:]\n",
    "\n",
    "    # Iterating through the data and structuring it for JSON\n",
    "    for t_index, time_value in enumerate(time):\n",
    "        for lat_index, lat_value in enumerate(latitudes):\n",
    "            for lon_index, lon_value in enumerate(longitudes):\n",
    "                data_dict[\"data\"][\"tempanomaly\"].append({\n",
    "                    \"time\": str(time_value),  # Convert to string or appropriate format\n",
    "                    \"lat\": str(lat_value),\n",
    "                    \"lon\": str(lon_value),\n",
    "                    \"value\": tempanomalies[t_index, lat_index, lon_index].item()  # Convert numpy data to native Python\n",
    "                })\n",
    "    \n",
    "    # Write the data to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=4)\n",
    "    \n",
    "    dataset.close()\n",
    "\n",
    "# Specify your .nc file path and the desired output .json file path\n",
    "nc_file_path = 'gistemp250_GHCNv4.nc'\n",
    "json_file_path = 'gistemp250_GHCNv4.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a7514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to run this code on your local machine\n",
    "# convert_nc_to_json(nc_file_path, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511a0da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '23 years',\n",
       " 'All years',\n",
       " 'done',\n",
       " 'gistemp250_GHCNv4.nc',\n",
       " 'ncToJson.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat GPT Generated code to convert .nc into .json. \n",
    "# Chat GPT creates a new file for each time change in the .nc file.\n",
    "\n",
    "import netCDF4 as nc\n",
    "import json\n",
    "import os\n",
    "\n",
    "def convert_nc_to_separate_json_by_time(nc_file_path, output_folder_path):\n",
    "    dataset = nc.Dataset(nc_file_path)\n",
    "    \n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "    \n",
    "    time = dataset.variables['time'][:]\n",
    "    latitudes = dataset.variables['lat'][:]\n",
    "    longitudes = dataset.variables['lon'][:]\n",
    "    tempanomalies = dataset.variables['tempanomaly'][:]\n",
    "    \n",
    "    for t_index, time_value in enumerate(time):\n",
    "        data_for_time_step = {\n",
    "            \"time\": time_value.item(),  # Convert time to native Python type if necessary\n",
    "            \"tempanomaly\": []\n",
    "        }\n",
    "        \n",
    "        for lat_index, lat_value in enumerate(latitudes):\n",
    "            for lon_index, lon_value in enumerate(longitudes):\n",
    "                anomaly_value = tempanomalies[t_index, lat_index, lon_index].item()  # Ensure conversion to Python native type\n",
    "                data_for_time_step[\"tempanomaly\"].append({\n",
    "                    \"lat\": lat_value.item(),  # Ensure latitudes are converted\n",
    "                    \"lon\": lon_value.item(),  # Ensure longitudes are converted\n",
    "                    \"value\": anomaly_value  # This is now a native Python type\n",
    "                })\n",
    "        \n",
    "        json_filename = os.path.join(output_folder_path, f\"tempanomaly_time_{t_index}.json\")\n",
    "        \n",
    "        with open(json_filename, 'w') as json_file:\n",
    "            json.dump(data_for_time_step, json_file, indent=4)\n",
    "    \n",
    "    dataset.close()\n",
    "\n",
    "\n",
    "# Remember to replace the paths above with actual paths before running the script\n",
    "\n",
    "# Specify the path to your .nc file and the output directory\n",
    "nc_file_path = 'gistemp250_GHCNv4.nc'  # Update with the actual path to your .nc file\n",
    "output_folder_path =  os.getcwd() # Update with your desired output directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1d910e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute the function with your paths\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m convert_nc_to_separate_json_by_time(nc_file_path, output_folder_path)\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mconvert_nc_to_separate_json_by_time\u001b[1;34m(nc_file_path, output_folder_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m     json_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempanomaly_time_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m---> 37\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data_for_time_step, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     39\u001b[0m dataset\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to execute the function with your paths\n",
    "# convert_nc_to_separate_json_by_time(nc_file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce66d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tempanomaly_time_1450.json',\n",
       " 'tempanomaly_time_1451.json',\n",
       " 'tempanomaly_time_1452.json',\n",
       " 'tempanomaly_time_1453.json',\n",
       " 'tempanomaly_time_1454.json',\n",
       " 'tempanomaly_time_1455.json',\n",
       " 'tempanomaly_time_1456.json',\n",
       " 'tempanomaly_time_1457.json',\n",
       " 'tempanomaly_time_1458.json',\n",
       " 'tempanomaly_time_1459.json',\n",
       " 'tempanomaly_time_1460.json',\n",
       " 'tempanomaly_time_1461.json',\n",
       " 'tempanomaly_time_1462.json',\n",
       " 'tempanomaly_time_1463.json',\n",
       " 'tempanomaly_time_1464.json',\n",
       " 'tempanomaly_time_1465.json',\n",
       " 'tempanomaly_time_1466.json',\n",
       " 'tempanomaly_time_1467.json',\n",
       " 'tempanomaly_time_1468.json',\n",
       " 'tempanomaly_time_1469.json',\n",
       " 'tempanomaly_time_1470.json',\n",
       " 'tempanomaly_time_1471.json',\n",
       " 'tempanomaly_time_1472.json',\n",
       " 'tempanomaly_time_1473.json',\n",
       " 'tempanomaly_time_1474.json',\n",
       " 'tempanomaly_time_1475.json',\n",
       " 'tempanomaly_time_1476.json',\n",
       " 'tempanomaly_time_1477.json',\n",
       " 'tempanomaly_time_1478.json',\n",
       " 'tempanomaly_time_1479.json',\n",
       " 'tempanomaly_time_1480.json',\n",
       " 'tempanomaly_time_1481.json',\n",
       " 'tempanomaly_time_1482.json',\n",
       " 'tempanomaly_time_1483.json',\n",
       " 'tempanomaly_time_1484.json',\n",
       " 'tempanomaly_time_1485.json',\n",
       " 'tempanomaly_time_1486.json',\n",
       " 'tempanomaly_time_1487.json',\n",
       " 'tempanomaly_time_1488.json',\n",
       " 'tempanomaly_time_1489.json',\n",
       " 'tempanomaly_time_1490.json',\n",
       " 'tempanomaly_time_1491.json',\n",
       " 'tempanomaly_time_1492.json',\n",
       " 'tempanomaly_time_1493.json',\n",
       " 'tempanomaly_time_1494.json',\n",
       " 'tempanomaly_time_1495.json',\n",
       " 'tempanomaly_time_1496.json',\n",
       " 'tempanomaly_time_1497.json',\n",
       " 'tempanomaly_time_1498.json',\n",
       " 'tempanomaly_time_1499.json',\n",
       " 'tempanomaly_time_1500.json',\n",
       " 'tempanomaly_time_1501.json',\n",
       " 'tempanomaly_time_1502.json',\n",
       " 'tempanomaly_time_1503.json',\n",
       " 'tempanomaly_time_1504.json',\n",
       " 'tempanomaly_time_1505.json',\n",
       " 'tempanomaly_time_1506.json',\n",
       " 'tempanomaly_time_1507.json',\n",
       " 'tempanomaly_time_1508.json',\n",
       " 'tempanomaly_time_1509.json',\n",
       " 'tempanomaly_time_1510.json',\n",
       " 'tempanomaly_time_1511.json',\n",
       " 'tempanomaly_time_1512.json',\n",
       " 'tempanomaly_time_1513.json',\n",
       " 'tempanomaly_time_1514.json',\n",
       " 'tempanomaly_time_1515.json',\n",
       " 'tempanomaly_time_1516.json',\n",
       " 'tempanomaly_time_1517.json',\n",
       " 'tempanomaly_time_1518.json',\n",
       " 'tempanomaly_time_1519.json',\n",
       " 'tempanomaly_time_1520.json',\n",
       " 'tempanomaly_time_1521.json',\n",
       " 'tempanomaly_time_1522.json',\n",
       " 'tempanomaly_time_1523.json',\n",
       " 'tempanomaly_time_1524.json',\n",
       " 'tempanomaly_time_1525.json',\n",
       " 'tempanomaly_time_1526.json',\n",
       " 'tempanomaly_time_1527.json',\n",
       " 'tempanomaly_time_1528.json',\n",
       " 'tempanomaly_time_1529.json',\n",
       " 'tempanomaly_time_1530.json',\n",
       " 'tempanomaly_time_1531.json',\n",
       " 'tempanomaly_time_1532.json',\n",
       " 'tempanomaly_time_1533.json',\n",
       " 'tempanomaly_time_1534.json',\n",
       " 'tempanomaly_time_1535.json',\n",
       " 'tempanomaly_time_1536.json',\n",
       " 'tempanomaly_time_1537.json',\n",
       " 'tempanomaly_time_1538.json',\n",
       " 'tempanomaly_time_1539.json',\n",
       " 'tempanomaly_time_1540.json',\n",
       " 'tempanomaly_time_1541.json',\n",
       " 'tempanomaly_time_1542.json',\n",
       " 'tempanomaly_time_1543.json',\n",
       " 'tempanomaly_time_1544.json',\n",
       " 'tempanomaly_time_1545.json',\n",
       " 'tempanomaly_time_1546.json',\n",
       " 'tempanomaly_time_1547.json',\n",
       " 'tempanomaly_time_1548.json',\n",
       " 'tempanomaly_time_1549.json',\n",
       " 'tempanomaly_time_1550.json',\n",
       " 'tempanomaly_time_1551.json',\n",
       " 'tempanomaly_time_1552.json',\n",
       " 'tempanomaly_time_1553.json',\n",
       " 'tempanomaly_time_1554.json',\n",
       " 'tempanomaly_time_1555.json',\n",
       " 'tempanomaly_time_1556.json',\n",
       " 'tempanomaly_time_1557.json',\n",
       " 'tempanomaly_time_1558.json',\n",
       " 'tempanomaly_time_1559.json',\n",
       " 'tempanomaly_time_1560.json',\n",
       " 'tempanomaly_time_1561.json',\n",
       " 'tempanomaly_time_1562.json',\n",
       " 'tempanomaly_time_1563.json',\n",
       " 'tempanomaly_time_1564.json',\n",
       " 'tempanomaly_time_1565.json',\n",
       " 'tempanomaly_time_1566.json',\n",
       " 'tempanomaly_time_1567.json',\n",
       " 'tempanomaly_time_1568.json',\n",
       " 'tempanomaly_time_1569.json',\n",
       " 'tempanomaly_time_1570.json',\n",
       " 'tempanomaly_time_1571.json',\n",
       " 'tempanomaly_time_1572.json',\n",
       " 'tempanomaly_time_1573.json',\n",
       " 'tempanomaly_time_1574.json',\n",
       " 'tempanomaly_time_1575.json',\n",
       " 'tempanomaly_time_1576.json',\n",
       " 'tempanomaly_time_1577.json',\n",
       " 'tempanomaly_time_1578.json',\n",
       " 'tempanomaly_time_1579.json',\n",
       " 'tempanomaly_time_1580.json',\n",
       " 'tempanomaly_time_1581.json',\n",
       " 'tempanomaly_time_1582.json',\n",
       " 'tempanomaly_time_1583.json',\n",
       " 'tempanomaly_time_1584.json',\n",
       " 'tempanomaly_time_1585.json',\n",
       " 'tempanomaly_time_1586.json',\n",
       " 'tempanomaly_time_1587.json',\n",
       " 'tempanomaly_time_1588.json',\n",
       " 'tempanomaly_time_1589.json',\n",
       " 'tempanomaly_time_1590.json',\n",
       " 'tempanomaly_time_1591.json',\n",
       " 'tempanomaly_time_1592.json',\n",
       " 'tempanomaly_time_1593.json',\n",
       " 'tempanomaly_time_1594.json',\n",
       " 'tempanomaly_time_1595.json',\n",
       " 'tempanomaly_time_1596.json',\n",
       " 'tempanomaly_time_1597.json',\n",
       " 'tempanomaly_time_1598.json',\n",
       " 'tempanomaly_time_1599.json',\n",
       " 'tempanomaly_time_1600.json',\n",
       " 'tempanomaly_time_1601.json',\n",
       " 'tempanomaly_time_1602.json',\n",
       " 'tempanomaly_time_1603.json',\n",
       " 'tempanomaly_time_1604.json',\n",
       " 'tempanomaly_time_1605.json',\n",
       " 'tempanomaly_time_1606.json',\n",
       " 'tempanomaly_time_1607.json',\n",
       " 'tempanomaly_time_1608.json',\n",
       " 'tempanomaly_time_1609.json',\n",
       " 'tempanomaly_time_1610.json',\n",
       " 'tempanomaly_time_1611.json',\n",
       " 'tempanomaly_time_1612.json',\n",
       " 'tempanomaly_time_1613.json',\n",
       " 'tempanomaly_time_1614.json',\n",
       " 'tempanomaly_time_1615.json',\n",
       " 'tempanomaly_time_1616.json',\n",
       " 'tempanomaly_time_1617.json',\n",
       " 'tempanomaly_time_1618.json',\n",
       " 'tempanomaly_time_1619.json',\n",
       " 'tempanomaly_time_1620.json',\n",
       " 'tempanomaly_time_1621.json',\n",
       " 'tempanomaly_time_1622.json',\n",
       " 'tempanomaly_time_1623.json',\n",
       " 'tempanomaly_time_1624.json',\n",
       " 'tempanomaly_time_1625.json',\n",
       " 'tempanomaly_time_1626.json',\n",
       " 'tempanomaly_time_1627.json',\n",
       " 'tempanomaly_time_1628.json',\n",
       " 'tempanomaly_time_1629.json',\n",
       " 'tempanomaly_time_1630.json',\n",
       " 'tempanomaly_time_1631.json',\n",
       " 'tempanomaly_time_1632.json',\n",
       " 'tempanomaly_time_1633.json',\n",
       " 'tempanomaly_time_1634.json',\n",
       " 'tempanomaly_time_1635.json',\n",
       " 'tempanomaly_time_1636.json',\n",
       " 'tempanomaly_time_1637.json',\n",
       " 'tempanomaly_time_1638.json',\n",
       " 'tempanomaly_time_1639.json',\n",
       " 'tempanomaly_time_1640.json',\n",
       " 'tempanomaly_time_1641.json',\n",
       " 'tempanomaly_time_1642.json',\n",
       " 'tempanomaly_time_1643.json',\n",
       " 'tempanomaly_time_1644.json',\n",
       " 'tempanomaly_time_1645.json',\n",
       " 'tempanomaly_time_1646.json',\n",
       " 'tempanomaly_time_1647.json',\n",
       " 'tempanomaly_time_1648.json',\n",
       " 'tempanomaly_time_1649.json',\n",
       " 'tempanomaly_time_1650.json',\n",
       " 'tempanomaly_time_1651.json',\n",
       " 'tempanomaly_time_1652.json',\n",
       " 'tempanomaly_time_1653.json',\n",
       " 'tempanomaly_time_1654.json',\n",
       " 'tempanomaly_time_1655.json',\n",
       " 'tempanomaly_time_1656.json',\n",
       " 'tempanomaly_time_1657.json',\n",
       " 'tempanomaly_time_1658.json',\n",
       " 'tempanomaly_time_1659.json',\n",
       " 'tempanomaly_time_1660.json',\n",
       " 'tempanomaly_time_1661.json',\n",
       " 'tempanomaly_time_1662.json',\n",
       " 'tempanomaly_time_1663.json',\n",
       " 'tempanomaly_time_1664.json',\n",
       " 'tempanomaly_time_1665.json',\n",
       " 'tempanomaly_time_1666.json',\n",
       " 'tempanomaly_time_1667.json',\n",
       " 'tempanomaly_time_1668.json',\n",
       " 'tempanomaly_time_1669.json',\n",
       " 'tempanomaly_time_1670.json',\n",
       " 'tempanomaly_time_1671.json',\n",
       " 'tempanomaly_time_1672.json',\n",
       " 'tempanomaly_time_1673.json',\n",
       " 'tempanomaly_time_1674.json',\n",
       " 'tempanomaly_time_1675.json',\n",
       " 'tempanomaly_time_1676.json',\n",
       " 'tempanomaly_time_1677.json',\n",
       " 'tempanomaly_time_1678.json',\n",
       " 'tempanomaly_time_1679.json',\n",
       " 'tempanomaly_time_1680.json',\n",
       " 'tempanomaly_time_1681.json',\n",
       " 'tempanomaly_time_1682.json',\n",
       " 'tempanomaly_time_1683.json',\n",
       " 'tempanomaly_time_1684.json',\n",
       " 'tempanomaly_time_1685.json',\n",
       " 'tempanomaly_time_1686.json',\n",
       " 'tempanomaly_time_1687.json',\n",
       " 'tempanomaly_time_1688.json',\n",
       " 'tempanomaly_time_1689.json',\n",
       " 'tempanomaly_time_1690.json',\n",
       " 'tempanomaly_time_1691.json',\n",
       " 'tempanomaly_time_1692.json',\n",
       " 'tempanomaly_time_1693.json',\n",
       " 'tempanomaly_time_1694.json',\n",
       " 'tempanomaly_time_1695.json',\n",
       " 'tempanomaly_time_1696.json',\n",
       " 'tempanomaly_time_1697.json',\n",
       " 'tempanomaly_time_1698.json',\n",
       " 'tempanomaly_time_1699.json',\n",
       " 'tempanomaly_time_1700.json',\n",
       " 'tempanomaly_time_1701.json',\n",
       " 'tempanomaly_time_1702.json',\n",
       " 'tempanomaly_time_1703.json',\n",
       " 'tempanomaly_time_1704.json',\n",
       " 'tempanomaly_time_1705.json',\n",
       " 'tempanomaly_time_1706.json',\n",
       " 'tempanomaly_time_1707.json',\n",
       " 'tempanomaly_time_1708.json',\n",
       " 'tempanomaly_time_1709.json',\n",
       " 'tempanomaly_time_1710.json',\n",
       " 'tempanomaly_time_1711.json',\n",
       " 'tempanomaly_time_1712.json',\n",
       " 'tempanomaly_time_1713.json',\n",
       " 'tempanomaly_time_1714.json',\n",
       " 'tempanomaly_time_1715.json',\n",
       " 'tempanomaly_time_1716.json',\n",
       " 'tempanomaly_time_1717.json',\n",
       " 'tempanomaly_time_1718.json',\n",
       " 'tempanomaly_time_1719.json',\n",
       " 'tempanomaly_time_1720.json',\n",
       " 'tempanomaly_time_1721.json',\n",
       " 'tempanomaly_time_1722.json',\n",
       " 'tempanomaly_time_1723.json',\n",
       " 'tempanomaly_time_1724.json',\n",
       " 'tempanomaly_time_1725.json',\n",
       " 'tempanomaly_time_2312.json',\n",
       " 'tempanomaly_time_2401.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "directory = os.listdir('23 years')\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88707001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_time_in_json(json_file_path):\n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # The reference start date: January 1, 1880\n",
    "    start_date = datetime(1900, 1, 31)\n",
    "    \n",
    "    # Convert 'time' to yy/mm/dd\n",
    "    days_since_start = timedelta(days=data['time'])\n",
    "    actual_date = start_date + days_since_start\n",
    "    \n",
    "    # Update the 'time' field with the formatted date string\n",
    "    data['time'] = actual_date.strftime('%y/%m/%d')\n",
    "    \n",
    "    # For demonstration purposes, print the updated data\n",
    "    #print(json.dumps(data, indent=4))\n",
    "    print(data['time'])\n",
    "    # Optionally, write the updated data back to the file or a new file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    \n",
    "# Example usage\n",
    "#list dir = '/23 years'\n",
    "os.chdir('23 years')\n",
    "for i in directory:\n",
    "    json_file_path = i # Update this path to your actual JSON file location\n",
    "    print(i)\n",
    "    convert_time_in_json(json_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11266f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
